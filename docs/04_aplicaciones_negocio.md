# MÃ³dulo 4: Aplicaciones de Negocio

## ðŸ“– Ãndice

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Del Prototipo a ProducciÃ³n](#producciÃ³n)
3. [Casos de Uso Empresariales](#casos-de-uso)
4. [Arquitecturas de Sistemas Reales](#arquitecturas)
5. [Monitoreo y Observabilidad](#monitoreo)
6. [OptimizaciÃ³n de Costos](#costos)
7. [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)

---

## ðŸŽ¯ IntroducciÃ³n {#introducciÃ³n}

En este mÃ³dulo aplicamos todo lo aprendido a **casos de uso reales de negocio**. Los ejercicios estÃ¡n basados en sistemas que se usan en producciÃ³n.

### Diferencia: Ejercicios vs Aplicaciones Reales

| Aspecto | Ejercicios (M1-M3) | Aplicaciones (M4) |
|---------|-------------------|-------------------|
| **PropÃ³sito** | Aprender patterns | Resolver problemas reales |
| **Complejidad** | Un pattern a la vez | MÃºltiples patterns integrados |
| **Datos** | Simulados | Realistas |
| **Errores** | Controlados | Todos los edge cases |
| **Escalabilidad** | No crÃ­tica | CrÃ­tica |
| **Monitoreo** | Opcional | Esencial |

---

## ðŸš€ Del Prototipo a ProducciÃ³n {#producciÃ³n}

### Checklist de ProducciÃ³n

#### 1. Funcionalidad âœ…
- [ ] Todos los flujos principales funcionan
- [ ] Casos de error manejados
- [ ] Validaciones de entrada
- [ ] Tests comprehensivos (>80% cobertura)

#### 2. Rendimiento âš¡
- [ ] Latencia < requisitos (ej: p95 < 3s)
- [ ] Throughput adecuado
- [ ] Concurrencia sin degradaciÃ³n
- [ ] Timeout configurados

#### 3. Confiabilidad ðŸ”’
- [ ] Retry con backoff exponencial
- [ ] Circuit breakers
- [ ] Fallbacks para LLM failures
- [ ] Idempotencia en operaciones crÃ­ticas

#### 4. Observabilidad ðŸ‘ï¸
- [ ] Logging estructurado
- [ ] MÃ©tricas (latencia, errores, costos)
- [ ] Tracing distribuido
- [ ] Alertas configuradas

#### 5. Seguridad ðŸ›¡ï¸
- [ ] API keys en secrets manager
- [ ] Input sanitization
- [ ] Rate limiting
- [ ] Audit logs

#### 6. Costos ðŸ’°
- [ ] Presupuesto definido
- [ ] Monitoring de costos por llamada
- [ ] Caching donde aplique
- [ ] Modelos optimizados (gpt-4o-mini donde sea suficiente)

---

## ðŸ’¼ Casos de Uso Empresariales {#casos-de-uso}

### 1. AtenciÃ³n al Cliente (Ejercicio 4.1)

**Problema de Negocio:**
- Volumen de consultas: 500-1,000/dÃ­a
- Costo por agente humano: $15/hora
- Tiempo promedio por consulta: 10 min
- Costo mensual: $50,000

**SoluciÃ³n con Agentes:**
- Automatiza 70% de consultas
- Tiempo de respuesta: < 30 segundos
- Costo por consulta: $0.05 (LLM)
- Ahorro mensual: $30,000

**ROI:**
- InversiÃ³n inicial: $20,000 (desarrollo)
- Payback: < 1 mes
- ROI aÃ±o 1: 1,500%

**MÃ©tricas Clave:**
```python
metrics = {
    "automation_rate": 0.72,  # 72% automated
    "escalation_rate": 0.28,  # 28% to humans
    "avg_confidence": 0.81,
    "csat_score": 4.3,  # /5
    "cost_per_query": 0.05  # USD
}
```

### 2. AnÃ¡lisis de Documentos (Ejercicio 4.2)

**Problema de Negocio:**
- AnÃ¡lisis manual: 2-4 horas/documento
- Analista senior: $80/hora
- Costo por documento: $160-320
- 50 documentos/mes = $8,000-16,000

**SoluciÃ³n con Agentes:**
- AnÃ¡lisis automatizado: 3 minutos
- 90% accuracy en extracciÃ³n
- Costo por documento: $2-5
- Ahorro mensual: $7,500-15,000

**Aplicaciones:**
- Legal: Contratos, acuerdos
- Finanzas: AnÃ¡lisis de propuestas
- Compliance: RevisiÃ³n de polÃ­ticas
- RFPs: EvaluaciÃ³n de proveedores

### 3. InvestigaciÃ³n Empresarial (Ejercicio 4.3)

**Problema de Negocio:**
- InvestigaciÃ³n manual: 4-8 horas
- Consultor: $150/hora
- Costo por investigaciÃ³n: $600-1,200
- Calidad inconsistente

**SoluciÃ³n con Agentes:**
- InvestigaciÃ³n automatizada: 10-15 minutos
- Costo: $5-10
- Calidad estandarizada
- Ahorro: 95% en tiempo y costo

**Aplicaciones:**
- Market research
- Competitive intelligence
- Technology assessment
- Due diligence preliminar

---

## ðŸ—ï¸ Arquitecturas de Sistemas Reales {#arquitecturas}

### Arquitectura de Microservicios

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FRONTEND (Web/Mobile)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ REST/GraphQL
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API GATEWAY                                    â”‚
â”‚  - Auth                                         â”‚
â”‚  - Rate Limiting                                â”‚
â”‚  - Request Routing                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“        â†“        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service  â”‚ â”‚ Service  â”‚ â”‚ Service  â”‚
â”‚ A        â”‚ â”‚ B        â”‚ â”‚ C        â”‚
â”‚(Customer)â”‚ â”‚(Document)â”‚ â”‚(Research)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“         â†“          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangGraphâ”‚ â”‚ Vector â”‚ â”‚ Cache    â”‚
â”‚ Runtime  â”‚ â”‚ DB     â”‚ â”‚ (Redis)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM APIs â”‚
â”‚ (OpenAI) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejemplo de ConfiguraciÃ³n

```python
# config.py
from pydantic import BaseSettings

class Settings(BaseSettings):
    # LLM
    openai_api_key: str
    openai_model: str = "gpt-4o-mini"
    openai_temperature: float = 0.3
    openai_max_tokens: int = 2000

    # Redis Cache
    redis_url: str
    cache_ttl: int = 3600  # 1 hour

    # Vector DB
    pinecone_api_key: str
    pinecone_environment: str
    pinecone_index: str

    # Monitoring
    langsmith_api_key: str
    langsmith_project: str

    # Rate Limits
    max_requests_per_minute: int = 60
    max_concurrent_requests: int = 10

    # Timeouts
    llm_timeout: int = 30
    pipeline_timeout: int = 120

    class Config:
        env_file = ".env"

settings = Settings()
```

---

## ðŸ‘ï¸ Monitoreo y Observabilidad {#monitoreo}

### MÃ©tricas Clave

#### 1. MÃ©tricas de Sistema

```python
from prometheus_client import Counter, Histogram, Gauge

# Requests
request_counter = Counter(
    'agent_requests_total',
    'Total agent requests',
    ['agent_type', 'status']
)

# Latencia
request_duration = Histogram(
    'agent_request_duration_seconds',
    'Request duration',
    ['agent_type']
)

# Costos
cost_gauge = Gauge(
    'agent_cost_usd',
    'Cost in USD',
    ['agent_type']
)

# Errores
error_counter = Counter(
    'agent_errors_total',
    'Total errors',
    ['agent_type', 'error_type']
)
```

#### 2. MÃ©tricas de Negocio

```python
# Automation rate
automation_rate = Gauge(
    'support_automation_rate',
    'Percentage of automated queries'
)

# Confidence scores
confidence_histogram = Histogram(
    'agent_confidence_score',
    'Confidence scores',
    buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
)

# Customer satisfaction
csat_gauge = Gauge(
    'customer_satisfaction_score',
    'CSAT score'
)
```

### Logging Estructurado

```python
import structlog

logger = structlog.get_logger()

def process_query(query_id: str, query: str):
    log = logger.bind(
        query_id=query_id,
        query_length=len(query)
    )

    log.info("query_received")

    try:
        result = agent.invoke({"query": query})

        log.info(
            "query_processed",
            confidence=result["confidence"],
            escalated=result["escalate"],
            duration_ms=elapsed_ms
        )

        return result

    except Exception as e:
        log.error(
            "query_failed",
            error=str(e),
            error_type=type(e).__name__
        )
        raise
```

### Tracing con LangSmith

**LangSmith** es la plataforma oficial de observabilidad para aplicaciones LangChain/LangGraph. Proporciona:

- âœ… **Tracing automÃ¡tico** de todos los componentes
- ðŸ“Š **MÃ©tricas detalladas** (latencia, tokens, costos)
- ðŸ” **Debugging visual** de flujos de agentes
- ðŸ“ˆ **EvaluaciÃ³n y comparaciÃ³n** de versiones
- ðŸš¨ **Alertas** en tiempo real

#### ConfiguraciÃ³n BÃ¡sica

```python
# .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls__your_api_key
LANGCHAIN_PROJECT=production-support-agent
```

#### Tracing AutomÃ¡tico

```python
from langsmith import Client
from utils.langsmith_config import get_runnable_config, add_run_metadata

client = Client()

# Todo cÃ³digo LangGraph se traza automÃ¡ticamente
def process_customer_query(query: str, user_id: str):
    # Configurar tags y metadata para filtrado
    config = get_runnable_config(
        tags=["production", "customer-support", "v2.0"],
        metadata={
            "user_id": user_id,
            "session_id": get_session_id(),
            "environment": "prod"
        },
        run_name=f"SupportQuery_{user_id}"
    )

    # Esta llamada se traza automÃ¡ticamente con todo el contexto
    result = agent.invoke({"query": query}, config=config)

    # AÃ±adir metadata adicional despuÃ©s de la ejecuciÃ³n
    add_run_metadata({
        "escalated": result.get("escalate", False),
        "confidence": result.get("confidence", 0),
        "response_length": len(result.get("response", ""))
    })

    return result
```

#### Logging de Decisiones para Debugging

```python
from utils.langsmith_config import log_agent_decision

def routing_agent(state):
    intent = classify_intent(state["query"])

    # Registrar decisiÃ³n para debugging posterior
    log_agent_decision(
        agent_name="Router",
        decision=f"route_to_{intent}",
        reasoning=f"Query classified as {intent} with keywords: {keywords}",
        confidence=classification_confidence
    )

    return {"intent": intent}
```

#### AnÃ¡lisis en LangSmith UI

En la interfaz de LangSmith puedes:

1. **Ver traces en tiempo real**
   - Flujo completo del grafo (nodos y edges)
   - Prompts exactos enviados al LLM
   - Respuestas completas
   - Tiempo de cada paso

2. **Filtrar y buscar**
   ```
   # Buscar por tags
   tag:production AND tag:customer-support

   # Buscar por metadata
   metadata.user_id = "user123"

   # Buscar por contenido
   "billing question"

   # Buscar errores
   status:error
   ```

3. **Comparar versiones**
   - A/B testing de prompts
   - Comparar latencia entre versiones
   - Medir impacto de cambios

4. **Analizar mÃ©tricas**
   - Latencia (p50, p95, p99)
   - Tokens y costos
   - Error rates
   - Success rates

#### Monitoreo de ProducciÃ³n con LangSmith

```python
from langsmith import Client
from datetime import datetime, timedelta

def monitor_production_health():
    """
    Monitorea la salud del sistema en producciÃ³n.
    """
    client = Client()

    # Obtener runs de las Ãºltimas 24 horas
    runs = client.list_runs(
        project_name="production-support-agent",
        start_time=datetime.now() - timedelta(days=1),
        filter='tag:production'
    )

    # Calcular mÃ©tricas
    total = len(list(runs))
    errors = sum(1 for r in runs if r.error)
    avg_latency = sum(r.latency for r in runs if r.latency) / total

    error_rate = errors / total if total > 0 else 0

    # Alertar si hay problemas
    if error_rate > 0.05:  # 5% error rate
        alert_team(f"âš ï¸ High error rate: {error_rate:.1%}")

    if avg_latency > 5000:  # 5 segundos
        alert_team(f"âš ï¸ High latency: {avg_latency:.0f}ms")

    return {
        "total_requests": total,
        "error_rate": error_rate,
        "avg_latency_ms": avg_latency
    }
```

#### EvaluaciÃ³n Continua

```python
from langsmith import Client
from langsmith.evaluation import evaluate

client = Client()

# Crear dataset de casos de test
dataset_name = "support-agent-golden-set"

# Evaluar contra el dataset
def accuracy_evaluator(run, example):
    """EvalÃºa si la respuesta es correcta."""
    predicted_intent = run.outputs.get("intent")
    expected_intent = example.outputs.get("expected_intent")

    return {
        "key": "intent_accuracy",
        "score": int(predicted_intent == expected_intent)
    }

# Ejecutar evaluaciÃ³n
results = evaluate(
    lambda inputs: agent.invoke(inputs),
    data=dataset_name,
    evaluators=[accuracy_evaluator],
    experiment_prefix="support-agent-v2"
)

print(f"Accuracy: {results['accuracy']:.1%}")
```

#### Mejores PrÃ¡cticas de Observabilidad

1. **Tags Consistentes**
   ```python
   # Usar estrategia de tags definida
   tags = [
       "production",              # Ambiente
       "v2.1",                    # VersiÃ³n
       "customer-support",        # Dominio
       "high-priority"            # Criticidad
   ]
   ```

2. **Metadata Rico**
   ```python
   metadata = {
       "user_id": user_id,
       "session_id": session_id,
       "feature_flags": enabled_flags,
       "model_config": {
           "model": "gpt-4o-mini",
           "temperature": 0.7
       }
   }
   ```

3. **Naming Descriptivo**
   ```python
   # Nombres que describen la operaciÃ³n
   run_name = f"SupportTicket_{ticket_id}_Classification"
   ```

4. **Secciones LÃ³gicas**
   ```python
   from utils.langsmith_config import trace_section

   with trace_section("UserAuthentication", tags=["auth"]):
       user = authenticate(credentials)

   with trace_section("QueryProcessing", tags=["llm"]):
       response = process_query(user_query)

   with trace_section("ResponseValidation", tags=["validation"]):
       validated = validate_response(response)
   ```

Para mÃ¡s detalles sobre debugging con LangSmith, ver:
- ðŸ“š [DocumentaciÃ³n completa](../docs/05_debugging_langsmith.md)
- ðŸ’¡ [Ejemplo bÃ¡sico](../ejemplos/debugging_langsmith.py)
- ðŸŽ¯ [Ejercicio 4.4](../ejercicios/modulo_4/ejercicio_4_4_debugging/)

### Dashboards

```yaml
# Grafana Dashboard Config
dashboard:
  title: "LangGraph Agents - Production"

  panels:
    - title: "Requests per Minute"
      metric: rate(agent_requests_total[1m])
      type: graph

    - title: "p95 Latency"
      metric: histogram_quantile(0.95, agent_request_duration_seconds)
      type: singlestat

    - title: "Error Rate"
      metric: rate(agent_errors_total[5m]) / rate(agent_requests_total[5m])
      type: graph
      alert_threshold: 0.05  # 5%

    - title: "Cost per Hour"
      metric: sum(rate(agent_cost_usd[1h]))
      type: singlestat

    - title: "Automation Rate"
      metric: support_automation_rate
      type: gauge
```

---

## ðŸ’° OptimizaciÃ³n de Costos {#costos}

### Estrategias de OptimizaciÃ³n

#### 1. SelecciÃ³n de Modelo

```python
# Routing basado en complejidad
def select_model(query_complexity: str) -> str:
    if query_complexity == "simple":
        return "gpt-4o-mini"  # $0.15/1M tokens
    elif query_complexity == "medium":
        return "gpt-4o"  # $2.50/1M tokens
    else:
        return "gpt-4"  # $30/1M tokens

# Ahorro: 80% en consultas simples
```

#### 2. Caching Inteligente

```python
import hashlib
from functools import lru_cache

def cache_key(query: str, context: str) -> str:
    content = f"{query}:{context}"
    return hashlib.md5(content.encode()).hexdigest()

@lru_cache(maxsize=1000)
def cached_llm_call(cache_key: str, prompt: str):
    return llm.invoke(prompt)

# Ahorro: 30-40% con cache hit rate >50%
```

#### 3. Batch Processing

```python
# Procesar mÃºltiples queries en paralelo
async def process_batch(queries: List[str]):
    tasks = [agent.ainvoke({"query": q}) for q in queries]
    results = await asyncio.gather(*tasks)
    return results

# Throughput: 10x mejora
```

#### 4. Prompt Optimization

```python
# MAL: Prompt verboso (500 tokens)
prompt_bad = f"""
You are a highly skilled and experienced customer support agent
with extensive knowledge of our products and services. Please
analyze the following customer query in great detail and provide
a comprehensive, thoughtful response...

Query: {query}
"""

# BIEN: Prompt conciso (50 tokens)
prompt_good = f"""Respond to customer query:

{query}

Be concise and helpful."""

# Ahorro: 90% en prompt tokens
```

### CÃ¡lculo de Costos

```python
def estimate_costs(
    requests_per_day: int,
    avg_tokens_per_request: int,
    model: str = "gpt-4o-mini"
):
    # Precios (input + output)
    model_costs = {
        "gpt-4o-mini": 0.15 / 1_000_000,  # por token
        "gpt-4o": 2.50 / 1_000_000,
        "gpt-4": 30.00 / 1_000_000
    }

    cost_per_token = model_costs[model]
    tokens_per_day = requests_per_day * avg_tokens_per_request
    cost_per_day = tokens_per_day * cost_per_token

    return {
        "cost_per_day": cost_per_day,
        "cost_per_month": cost_per_day * 30,
        "cost_per_request": cost_per_day / requests_per_day
    }

# Ejemplo
costs = estimate_costs(
    requests_per_day=1000,
    avg_tokens_per_request=500,
    model="gpt-4o-mini"
)
# Output: {"cost_per_month": $2.25}
```

---

## âœ… Mejores PrÃ¡cticas {#mejores-prÃ¡cticas}

### 1. GestiÃ³n de Errores

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def resilient_llm_call(prompt: str):
    try:
        return llm.invoke(prompt)
    except RateLimitError:
        logger.warning("rate_limit_hit")
        raise  # Retry
    except APIError as e:
        logger.error("api_error", error=str(e))
        raise
    except Exception as e:
        logger.error("unexpected_error", error=str(e))
        # Fallback
        return fallback_response()
```

### 2. ValidaciÃ³n de Inputs

```python
from pydantic import BaseModel, validator

class CustomerQuery(BaseModel):
    query: str
    user_id: str

    @validator('query')
    def query_not_empty(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('Query cannot be empty')
        if len(v) > 5000:
            raise ValueError('Query too long (max 5000 chars)')
        return v.strip()

    @validator('user_id')
    def valid_user_id(cls, v):
        if not v.startswith('user_'):
            raise ValueError('Invalid user_id format')
        return v
```

### 3. Circuit Breaker

```python
from pybreaker import CircuitBreaker

llm_breaker = CircuitBreaker(
    fail_max=5,  # Open after 5 failures
    timeout_duration=60  # Stay open for 60s
)

@llm_breaker
def protected_llm_call(prompt: str):
    return llm.invoke(prompt)

try:
    result = protected_llm_call(prompt)
except CircuitBreakerError:
    # LLM service is down, use fallback
    result = fallback_service(prompt)
```

### 4. Testing en ProducciÃ³n

```python
# A/B Testing
def get_agent_variant(user_id: str) -> str:
    if hash(user_id) % 100 < 10:  # 10% traffic
        return "variant_b"
    return "variant_a"

# Feature Flags
from launchdarkly import LDClient

ld_client = LDClient(sdk_key="your-key")

def should_use_new_feature(user_id: str) -> bool:
    user = {"key": user_id}
    return ld_client.variation("new-agent-feature", user, False)
```

### 5. Deployment

```yaml
# docker-compose.yml
version: '3.8'

services:
  agent-api:
    image: company/agent-api:latest
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    ports:
      - "8000:8000"
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
```

---

## ðŸ“š Recursos Adicionales

- [LangGraph Production Guide](https://langchain-ai.github.io/langgraph/how-tos/production/)
- [LangSmith Monitoring](https://docs.smith.langchain.com/)
- [OpenAI Best Practices](https://platform.openai.com/docs/guides/production-best-practices)
- [Prometheus for LLMs](https://prometheus.io/)

---

## ðŸŽ“ Resumen del MÃ³dulo

Has aprendido a:
âœ… DiseÃ±ar sistemas multi-agente para casos reales
âœ… Implementar monitoring y observabilidad
âœ… Optimizar costos y rendimiento
âœ… Desplegar a producciÃ³n con confianza
âœ… Manejar errores y edge cases
âœ… Medir ROI y valor de negocio

**Â¡Felicitaciones por completar el tutorial!** ðŸŽ‰

Ahora tienes las habilidades para construir sistemas multi-agente de nivel producciÃ³n con LangGraph.

"""
Ejercicio 4.1: Sistema de Atenci√≥n al Cliente - SOLUCI√ìN COMPLETA

Sistema completo que integra routing, especializaci√≥n, KB search y escalamiento.
"""

from typing import TypedDict, List, Dict, Literal
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END

load_dotenv()

# =============================================================================
# ESTADO GLOBAL
# =============================================================================

class CustomerSupportState(TypedDict):
    """Estado compartido del sistema de atenci√≥n al cliente."""
    user_query: str                 # Consulta del usuario
    user_id: str                    # ID del usuario
    conversation_history: List[BaseMessage]  # Historial de mensajes
    category: str                   # "product", "support", "order"
    urgency: str                    # "low", "medium", "high"
    product_analysis: str           # An√°lisis del product agent
    support_analysis: str           # An√°lisis del support agent
    order_analysis: str             # An√°lisis del order agent
    kb_results: List[Dict]          # Resultados de knowledge base
    final_response: str             # Respuesta final al usuario
    confidence_score: float         # Score de confianza (0-1)
    should_escalate: bool           # Si escalar a humano
    escalation_reason: str          # Raz√≥n del escalamiento


llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)


# =============================================================================
# KNOWLEDGE BASE (Simulado)
# =============================================================================

knowledge_base = {
    "products": [
        {
            "id": "LAPTOP001",
            "name": "Laptop Pro X15",
            "price": 1299.99,
            "specs": "Intel i7 11th Gen, 16GB RAM DDR4, 512GB SSD NVMe, Intel Iris Xe Graphics, 15.6\" Full HD IPS",
            "warranty": "2 years",
            "category": "laptops",
            "stock": 25
        },
        {
            "id": "LAPTOP002",
            "name": "Laptop Pro X15 Gaming",
            "price": 1799.99,
            "specs": "Intel i7 11th Gen, 32GB RAM DDR4, 1TB SSD NVMe, NVIDIA RTX 3060 6GB, 15.6\" Full HD 144Hz",
            "warranty": "2 years",
            "category": "laptops",
            "stock": 12
        },
        {
            "id": "PHONE001",
            "name": "Smartphone Ultra Z",
            "price": 899.99,
            "specs": "Snapdragon 8 Gen 2, 12GB RAM, 256GB Storage, 6.7\" AMOLED 120Hz, 108MP Camera",
            "warranty": "1 year",
            "category": "phones",
            "stock": 50
        },
        {
            "id": "TABLET001",
            "name": "Tablet Pro 12",
            "price": 649.99,
            "specs": "Apple M1, 8GB RAM, 256GB Storage, 12.9\" Liquid Retina XDR",
            "warranty": "1 year",
            "category": "tablets",
            "stock": 18
        }
    ],
    "faqs": [
        {
            "question": "¬øCu√°l es la pol√≠tica de devoluciones?",
            "answer": "Aceptamos devoluciones dentro de 30 d√≠as de la compra sin preguntas. El producto debe estar en su empaque original y en condiciones de reventa. El reembolso se procesa en 5-7 d√≠as h√°biles.",
            "category": "policy"
        },
        {
            "question": "¬øCu√°nto tarda el env√≠o?",
            "answer": "Env√≠o est√°ndar: 5-7 d√≠as h√°biles. Env√≠o express: 2-3 d√≠as h√°biles. Env√≠o same-day disponible en Ciudad de M√©xico, Guadalajara y Monterrey para pedidos antes de las 2 PM.",
            "category": "shipping"
        },
        {
            "question": "¬øQu√© m√©todos de pago aceptan?",
            "answer": "Aceptamos: Tarjetas de cr√©dito/d√©bito (Visa, Mastercard, American Express), PayPal, transferencia bancaria, y pago en efectivo en tiendas OXXO.",
            "category": "payment"
        },
        {
            "question": "¬øC√≥mo puedo rastrear mi pedido?",
            "answer": "Recibir√°s un correo con el n√∫mero de rastreo 24 horas despu√©s del env√≠o. Puedes rastrear tu pedido en nuestra p√°gina de tracking o directamente en el sitio de la paqueter√≠a.",
            "category": "shipping"
        },
        {
            "question": "¬øLa garant√≠a cubre qu√©?",
            "answer": "La garant√≠a cubre defectos de fabricaci√≥n y fallas de hardware. NO cubre: da√±o por agua, ca√≠das, mal uso, o software de terceros. Incluye reparaci√≥n o reemplazo sin costo.",
            "category": "warranty"
        }
    ],
    "technical_docs": [
        {
            "product": "LAPTOP001",
            "issue": "No enciende",
            "solution": "1. Verificar que el cargador est√© conectado correctamente y el LED de carga est√© encendido. 2. Mantener presionado el bot√≥n de encendido por 10-15 segundos para hard reset. 3. Desconectar bater√≠a (si es removible), conectar solo el cargador y probar. 4. Verificar que no haya da√±o f√≠sico en puerto de carga. Si persiste, contactar soporte t√©cnico para diagn√≥stico avanzado."
        },
        {
            "product": "LAPTOP001",
            "issue": "Sobrecalentamiento",
            "solution": "1. Limpiar ventiladores con aire comprimido. 2. Usar en superficie dura y plana (no sobre cama/alfombra). 3. Actualizar drivers de gr√°ficos y BIOS. 4. Verificar apps con alto uso de CPU en Task Manager. 5. Considerar pasta t√©rmica nueva si tiene m√°s de 2 a√±os."
        },
        {
            "product": "PHONE001",
            "issue": "Bater√≠a se agota r√°pido",
            "solution": "1. Verificar apps con alto consumo en Settings > Battery > Battery Usage. 2. Reducir brillo de pantalla y activar modo adaptativo. 3. Desactivar servicios de ubicaci√≥n no necesarios. 4. Cerrar apps en segundo plano. 5. Activar modo de ahorro de energ√≠a. 6. Actualizar a √∫ltima versi√≥n de software. Si tiene m√°s de 2 a√±os, considerar reemplazo de bater√≠a."
        },
        {
            "product": "PHONE001",
            "issue": "No carga",
            "solution": "1. Probar con cable y cargador diferentes (usar originales si es posible). 2. Limpiar puerto de carga con aire comprimido (sin l√≠quidos). 3. Reiniciar el tel√©fono. 4. Probar carga inal√°mbrica si est√° disponible. 5. Verificar que cable no est√© da√±ado. Si no carga con m√∫ltiples cables/cargadores, contactar soporte."
        }
    ]
}


# =============================================================================
# FUNCIONES DE KNOWLEDGE BASE
# =============================================================================

def search_knowledge_base(query: str, category: str, kb: Dict = knowledge_base) -> List[Dict]:
    """
    Busca informaci√≥n relevante en la base de conocimiento.

    Implementa b√∫squeda simple por keywords. En producci√≥n,
    usar√≠as embeddings y b√∫squeda sem√°ntica.
    """
    results = []
    query_lower = query.lower()
    query_words = set(query_lower.split())

    # Buscar en productos si es consulta de producto
    if category == "product":
        for product in kb["products"]:
            # Buscar en nombre y specs
            product_text = f"{product['name']} {product['specs']}".lower()
            product_words = set(product_text.split())
            overlap = len(query_words & product_words)

            if overlap > 0:
                results.append({
                    "type": "product",
                    "data": product,
                    "relevance": overlap
                })

    # Buscar en docs t√©cnicas si es soporte
    elif category == "support":
        for doc in kb["technical_docs"]:
            doc_text = f"{doc['issue']} {doc['solution']}".lower()
            doc_words = set(doc_text.split())
            overlap = len(query_words & doc_words)

            if overlap > 1:
                results.append({
                    "type": "technical_doc",
                    "data": doc,
                    "relevance": overlap
                })

    # Siempre buscar en FAQs (son √∫tiles para todas las categor√≠as)
    for faq in kb["faqs"]:
        faq_text = f"{faq['question']} {faq['answer']}".lower()
        faq_words = set(faq_text.split())
        overlap = len(query_words & faq_words)

        if overlap > 1:
            results.append({
                "type": "faq",
                "data": faq,
                "relevance": overlap
            })

    # Ordenar por relevancia y retornar top 5
    results.sort(key=lambda x: x.get("relevance", 0), reverse=True)

    return results[:5]


# =============================================================================
# AGENTES
# =============================================================================

def intake_agent(state: CustomerSupportState) -> dict:
    """
    Agente inicial que clasifica la consulta y busca en KB.

    Este agente es el punto de entrada del sistema y establece
    el contexto para todo el flujo posterior.
    """
    print("\n" + "="*70)
    print("üéØ INTAKE AGENT: Clasificando consulta...")
    print("="*70)

    query = state["user_query"]

    # Clasificaci√≥n usando LLM
    classification_prompt = f"""Eres un agente de clasificaci√≥n de consultas de atenci√≥n al cliente para TechStore (tienda de tecnolog√≠a).

CONSULTA DEL USUARIO:
{query}

Clasifica la consulta en UNA de estas categor√≠as:
- PRODUCT: Preguntas sobre productos, especificaciones, comparaciones, recomendaciones, precios
- SUPPORT: Problemas t√©cnicos, troubleshooting, c√≥mo usar productos, reparaciones
- ORDER: Estado de pedidos, tracking, devoluciones, facturaci√≥n, env√≠os

Tambi√©n determina el nivel de URGENCIA:
- LOW: Pregunta informativa, no urgente, exploraci√≥n
- MEDIUM: Problema que necesita resoluci√≥n pronto pero no es cr√≠tico
- HIGH: Problema cr√≠tico, cliente bloqueado, frustrado, o usa palabras como "urgente", "YA", "inmediatamente"

Responde en formato exacto:
CATEGORY: [PRODUCT/SUPPORT/ORDER]
URGENCY: [LOW/MEDIUM/HIGH]

CLASIFICACI√ìN:"""

    response = llm.invoke(classification_prompt)
    classification_text = response.content

    # Parsear respuesta
    lines = classification_text.strip().split('\n')
    category = "product"  # default
    urgency = "medium"    # default

    for line in lines:
        if "CATEGORY:" in line.upper():
            cat = line.split(":")[-1].strip().lower()
            if cat in ["product", "support", "order"]:
                category = cat
        elif "URGENCY:" in line.upper():
            urg = line.split(":")[-1].strip().lower()
            if urg in ["low", "medium", "high"]:
                urgency = urg

    # Buscar en knowledge base
    kb_results = search_knowledge_base(query, category)

    print(f"   ‚Üí Categor√≠a: {category.upper()}")
    print(f"   ‚Üí Urgencia: {urgency.upper()}")
    print(f"   ‚Üí KB Results: {len(kb_results)} resultados encontrados")

    if kb_results:
        print(f"   ‚Üí Top result: {kb_results[0]['type']}")

    return {
        "category": category,
        "urgency": urgency,
        "kb_results": kb_results
    }


def product_agent(state: CustomerSupportState) -> dict:
    """Agente especializado en consultas sobre productos."""
    print("\nüíª PRODUCT AGENT: Analizando consulta de producto...")

    query = state["user_query"]
    kb_results = state.get("kb_results", [])

    # Preparar contexto de productos de KB
    kb_context = ""
    if kb_results:
        kb_context = "\n\nINFORMACI√ìN DE PRODUCTOS EN BASE DE CONOCIMIENTO:\n"
        for result in kb_results:
            if result["type"] == "product":
                product = result["data"]
                kb_context += f"\n{product['name']} (ID: {product['id']})\n"
                kb_context += f"  Precio: ${product['price']}\n"
                kb_context += f"  Specs: {product['specs']}\n"
                kb_context += f"  Garant√≠a: {product['warranty']}\n"
                kb_context += f"  Stock: {product['stock']} unidades\n"
            elif result["type"] == "faq":
                faq = result["data"]
                kb_context += f"\nFAQ: {faq['question']}\n"
                kb_context += f"  Respuesta: {faq['answer']}\n"

    # An√°lisis de producto
    analysis_prompt = f"""Eres un especialista en productos de TechStore.

CONSULTA DEL USUARIO:
{query}

{kb_context}

Tu trabajo es:
1. Responder preguntas espec√≠ficas sobre productos
2. Comparar productos si se solicita
3. Proporcionar recomendaciones basadas en necesidades
4. Incluir informaci√≥n de specs, precio, garant√≠a
5. Mencionar disponibilidad de stock si es relevante

Genera un an√°lisis DETALLADO Y ESPEC√çFICO desde la perspectiva de productos.
Usa informaci√≥n del cat√°logo cuando est√© disponible.

AN√ÅLISIS DE PRODUCTOS:"""

    response = llm.invoke(analysis_prompt)
    analysis = response.content

    print(f"   ‚úì An√°lisis de producto generado ({len(analysis)} caracteres)")

    return {"product_analysis": analysis}


def support_agent(state: CustomerSupportState) -> dict:
    """Agente especializado en soporte t√©cnico."""
    print("\nüîß SUPPORT AGENT: Analizando problema t√©cnico...")

    query = state["user_query"]
    kb_results = state.get("kb_results", [])

    # Preparar contexto de documentaci√≥n t√©cnica
    kb_context = ""
    if kb_results:
        kb_context = "\n\nDOCUMENTACI√ìN T√âCNICA RELEVANTE:\n"
        for result in kb_results:
            if result["type"] == "technical_doc":
                doc = result["data"]
                kb_context += f"\nProducto: {doc['product']}\n"
                kb_context += f"  Problema: {doc['issue']}\n"
                kb_context += f"  Soluci√≥n: {doc['solution']}\n"
            elif result["type"] == "faq" and "warranty" in result["data"].get("category", ""):
                faq = result["data"]
                kb_context += f"\nPol√≠tica: {faq['question']}\n"
                kb_context += f"  {faq['answer']}\n"

    # An√°lisis de soporte
    analysis_prompt = f"""Eres un especialista en soporte t√©cnico de TechStore.

CONSULTA DEL USUARIO:
{query}

{kb_context}

Tu trabajo es:
1. Diagnosticar el problema t√©cnico
2. Proporcionar pasos de troubleshooting espec√≠ficos y numerados
3. Usar documentaci√≥n t√©cnica cuando est√© disponible
4. Ser claro y preciso en las instrucciones
5. Mencionar cu√°ndo es necesario contactar soporte avanzado

Genera un an√°lisis T√âCNICO DETALLADO con pasos accionables.

AN√ÅLISIS DE SOPORTE T√âCNICO:"""

    response = llm.invoke(analysis_prompt)
    analysis = response.content

    print(f"   ‚úì An√°lisis de soporte generado ({len(analysis)} caracteres)")

    return {"support_analysis": analysis}


def order_agent(state: CustomerSupportState) -> dict:
    """Agente especializado en consultas sobre √≥rdenes."""
    print("\nüì¶ ORDER AGENT: Analizando consulta de orden...")

    query = state["user_query"]
    kb_results = state.get("kb_results", [])

    # Preparar contexto de pol√≠ticas
    kb_context = ""
    if kb_results:
        kb_context = "\n\nPOL√çTICAS Y PROCEDIMIENTOS RELEVANTES:\n"
        for result in kb_results:
            if result["type"] == "faq":
                faq = result["data"]
                if faq["category"] in ["policy", "shipping", "payment", "warranty"]:
                    kb_context += f"\n{faq['question']}\n"
                    kb_context += f"  {faq['answer']}\n"

    # An√°lisis de orden
    analysis_prompt = f"""Eres un especialista en gesti√≥n de √≥rdenes de TechStore.

CONSULTA DEL USUARIO:
{query}

{kb_context}

Tu trabajo es:
1. Responder sobre estado de pedidos, tracking, env√≠os
2. Explicar proceso de devoluciones y reembolsos
3. Aclarar pol√≠ticas de env√≠o y pagos
4. Proporcionar informaci√≥n de contacto para casos especiales
5. Ser preciso con tiempos y procedimientos

Genera un an√°lisis DETALLADO sobre la gesti√≥n de la orden.
Si necesitas informaci√≥n espec√≠fica del sistema de √≥rdenes que no tienes,
ind√≠calo claramente.

AN√ÅLISIS DE ORDEN:"""

    response = llm.invoke(analysis_prompt)
    analysis = response.content

    print(f"   ‚úì An√°lisis de orden generado ({len(analysis)} caracteres)")

    return {"order_analysis": analysis}


def synthesizer_agent(state: CustomerSupportState) -> dict:
    """
    Sintetiza la respuesta final y decide si escalar.

    Este es el agente m√°s importante porque determina
    la calidad de la respuesta final y si se puede manejar
    autom√°ticamente o requiere intervenci√≥n humana.
    """
    print("\n" + "="*70)
    print("‚úÖ SYNTHESIZER: Generando respuesta final...")
    print("="*70)

    query = state["user_query"]
    category = state["category"]
    urgency = state["urgency"]
    kb_results = state.get("kb_results", [])

    # Obtener el an√°lisis relevante
    specialist_analysis = ""
    if category == "product":
        specialist_analysis = state.get("product_analysis", "")
    elif category == "support":
        specialist_analysis = state.get("support_analysis", "")
    elif category == "order":
        specialist_analysis = state.get("order_analysis", "")

    # Preparar contexto de KB para s√≠ntesis
    kb_summary = ""
    if kb_results:
        kb_summary = f"\n\nSe encontraron {len(kb_results)} recursos en la base de conocimiento."

    # S√≠ntesis de respuesta
    synthesis_prompt = f"""Eres un agente que genera respuestas finales profesionales para atenci√≥n al cliente de TechStore.

CONSULTA ORIGINAL DEL USUARIO:
{query}

AN√ÅLISIS DEL ESPECIALISTA ({category.upper()}):
{specialist_analysis}

{kb_summary}

URGENCIA: {urgency.upper()}

Genera una RESPUESTA FINAL PROFESIONAL que:

1. SALUDO: Comienza con "Estimado cliente,"

2. CUERPO PRINCIPAL:
   - Sea clara, espec√≠fica y directa
   - Use informaci√≥n del an√°lisis del especialista
   - Incluya todos los detalles relevantes
   - Sea estructurada (usa listas, secciones si es apropiado)
   - Proporcione pasos accionables cuando aplique

3. CIERRE:
   - Ofrezca ayuda adicional
   - Sea cort√©s y profesional
   - Firma: "Saludos, Sistema de Atenci√≥n TechStore"

IMPORTANTE:
- Si el an√°lisis menciona que necesita informaci√≥n de sistemas externos (√≥rdenes, RMA, etc.)
  que no tienes, incluye al final: [REQUIERE_ESCALAMIENTO]
- Si la urgencia es HIGH y el problema es complejo, incluye: [REQUIERE_ESCALAMIENTO]
- Si hay incertidumbre significativa, incluye: [REQUIERE_ESCALAMIENTO]

RESPUESTA FINAL:"""

    response = llm.invoke(synthesis_prompt)
    final_response = response.content

    # Calcular confidence score
    confidence = 0.0

    # Factor 1: ¬øHay resultados en KB? (+0.3)
    if kb_results:
        confidence += 0.3

    # Factor 2: ¬øEl an√°lisis es sustancial? (+0.3)
    if len(specialist_analysis) > 100:
        confidence += 0.3

    # Factor 3: ¬øNo hay palabras de incertidumbre? (+0.2)
    uncertainty_keywords = [
        "no estoy seguro", "podr√≠a ser", "tal vez", "posiblemente",
        "no tengo acceso", "necesito informaci√≥n", "requiere validaci√≥n"
    ]
    has_uncertainty = any(kw in specialist_analysis.lower() for kw in uncertainty_keywords)
    if not has_uncertainty:
        confidence += 0.2

    # Factor 4: ¬øUrgency no es HIGH? (+0.2)
    if urgency != "high":
        confidence += 0.2

    confidence = min(confidence, 1.0)

    # Decidir escalamiento
    should_escalate = False
    escalation_reason = ""

    # Criterio 1: Respuesta indica que requiere escalamiento
    if "[REQUIERE_ESCALAMIENTO]" in final_response:
        should_escalate = True
        escalation_reason = "La consulta requiere acceso a sistemas externos o validaci√≥n manual"
        # Remover el tag de la respuesta
        final_response = final_response.replace("[REQUIERE_ESCALAMIENTO]", "").strip()

    # Criterio 2: Confidence muy bajo
    elif confidence < 0.5:
        should_escalate = True
        escalation_reason = f"Confidence score muy bajo ({confidence:.2f}). Requiere revisi√≥n humana"

    # Criterio 3: HIGH urgency + confidence bajo
    elif urgency == "high" and confidence < 0.7:
        should_escalate = True
        escalation_reason = f"Alta urgencia con confidence moderado ({confidence:.2f}). Mejor que lo maneje un humano"

    print(f"   ‚Üí Confidence Score: {confidence:.2f}")
    print(f"   ‚Üí Decisi√≥n: {'ESCALAR A HUMANO' if should_escalate else 'RESPONDER DIRECTAMENTE'}")

    return {
        "final_response": final_response,
        "confidence_score": confidence,
        "should_escalate": should_escalate,
        "escalation_reason": escalation_reason
    }


def respond_node(state: CustomerSupportState) -> dict:
    """Nodo final que muestra la respuesta al usuario."""
    print("\n" + "="*70)
    print("üì® RESPUESTA AL USUARIO")
    print("="*70)
    print(state["final_response"])

    return {}


def escalate_node(state: CustomerSupportState) -> dict:
    """Nodo que escala a agente humano."""
    print("\n" + "="*70)
    print("üö® ESCALADO A AGENTE HUMANO")
    print("="*70)
    print(f"Raz√≥n: {state['escalation_reason']}\n")
    print("INFORMACI√ìN RECOPILADA PARA AGENTE HUMANO:")
    print(f"   ‚Ä¢ Usuario: {state['user_id']}")
    print(f"   ‚Ä¢ Categor√≠a: {state['category'].upper()}")
    print(f"   ‚Ä¢ Urgencia: {state['urgency'].upper()}")
    print(f"   ‚Ä¢ Confidence Score: {state['confidence_score']:.2f}")
    print(f"\n   ‚Ä¢ Consulta Original:")
    print(f"     {state['user_query']}")

    # Mostrar an√°lisis recopilado
    if state["category"] == "product" and state.get("product_analysis"):
        print(f"\n   ‚Ä¢ An√°lisis de Productos (prelim):")
        print(f"     {state['product_analysis'][:200]}...")
    elif state["category"] == "support" and state.get("support_analysis"):
        print(f"\n   ‚Ä¢ An√°lisis T√©cnico (prelim):")
        print(f"     {state['support_analysis'][:200]}...")
    elif state["category"] == "order" and state.get("order_analysis"):
        print(f"\n   ‚Ä¢ An√°lisis de Orden (prelim):")
        print(f"     {state['order_analysis'][:200]}...")

    print(f"\n   ‚Ä¢ Tiempo estimado de respuesta: {'Inmediato' if state['urgency'] == 'high' else '5-15 minutos'}")

    return {}


# =============================================================================
# FUNCIONES DE ROUTING
# =============================================================================

def route_to_specialist(state: CustomerSupportState) -> Literal["product", "support", "order"]:
    """Rutea al especialista apropiado seg√∫n categor√≠a."""
    category = state["category"]

    # Mapeo directo
    category_map = {
        "product": "product",
        "support": "support",
        "order": "order"
    }

    next_node = category_map.get(category, "product")

    print(f"\n   ‚Üí Routing a especialista: {next_node.upper()}")

    return next_node


def route_after_synthesis(state: CustomerSupportState) -> Literal["respond", "escalate"]:
    """Decide si responder directamente o escalar a humano."""
    should_escalate = state["should_escalate"]

    if should_escalate:
        return "escalate"
    else:
        return "respond"


# =============================================================================
# CONSTRUCCI√ìN DEL GRAFO
# =============================================================================

def build_graph():
    """
    Construye el grafo del sistema de atenci√≥n al cliente.

    Arquitectura:
    - Intake clasifica y prepara contexto
    - Router deriva a especialista apropiado
    - Especialista analiza en su dominio
    - Synthesizer integra y decide escalamiento
    - Router final decide responder o escalar
    """
    workflow = StateGraph(CustomerSupportState)

    # Agregar todos los nodos
    workflow.add_node("intake", intake_agent)
    workflow.add_node("product", product_agent)
    workflow.add_node("support", support_agent)
    workflow.add_node("order", order_agent)
    workflow.add_node("synthesizer", synthesizer_agent)
    workflow.add_node("respond", respond_node)
    workflow.add_node("escalate", escalate_node)

    # Entry point
    workflow.set_entry_point("intake")

    # Routing condicional a especialista
    workflow.add_conditional_edges(
        "intake",
        route_to_specialist,
        {
            "product": "product",
            "support": "support",
            "order": "order"
        }
    )

    # Todos los especialistas van a synthesizer
    workflow.add_edge("product", "synthesizer")
    workflow.add_edge("support", "synthesizer")
    workflow.add_edge("order", "synthesizer")

    # Routing condicional despu√©s de s√≠ntesis
    workflow.add_conditional_edges(
        "synthesizer",
        route_after_synthesis,
        {
            "respond": "respond",
            "escalate": "escalate"
        }
    )

    # Nodos finales
    workflow.add_edge("respond", END)
    workflow.add_edge("escalate", END)

    return workflow.compile()


# =============================================================================
# EJECUCI√ìN Y DEMO
# =============================================================================

def main():
    print("\n" + "="*70)
    print("üè™ SISTEMA DE ATENCI√ìN AL CLIENTE - TECHSTORE")
    print("="*70)
    print("\nEste sistema demuestra integraci√≥n de m√∫ltiples patterns:")
    print("  ‚Ä¢ Routing inteligente")
    print("  ‚Ä¢ Agentes especializados")
    print("  ‚Ä¢ Knowledge base search")
    print("  ‚Ä¢ Confidence scoring")
    print("  ‚Ä¢ Escalamiento autom√°tico\n")

    # Consultas de ejemplo que cubren diferentes escenarios
    queries = [
        ("user_001", "¬øLa Laptop Pro X15 es buena para dise√±o gr√°fico? ¬øCu√°nta RAM tiene?"),
        ("user_002", "Mi smartphone no carga, ya prob√© con diferentes cables. ¬øQu√© hago?"),
        ("user_003", "Quiero devolver un producto que compr√© hace 15 d√≠as. ¬øCu√°l es el proceso?"),
        ("user_004", "URGENTE: Mi laptop no enciende y tengo presentaci√≥n ma√±ana. ¬°Necesito ayuda YA!"),
    ]

    app = build_graph()

    for i, (user_id, query) in enumerate(queries, 1):
        print(f"\n{'='*70}")
        print(f"üí¨ CONSULTA {i}/{len(queries)} (Usuario: {user_id})")
        print(f"{'='*70}")
        print(f"Query: {query}")

        initial_state = {
            "user_query": query,
            "user_id": user_id,
            "conversation_history": [HumanMessage(content=query)],
            "category": "",
            "urgency": "",
            "product_analysis": "",
            "support_analysis": "",
            "order_analysis": "",
            "kb_results": [],
            "final_response": "",
            "confidence_score": 0.0,
            "should_escalate": False,
            "escalation_reason": ""
        }

        # Ejecutar grafo
        final_state = app.invoke(initial_state)

        # Mostrar m√©tricas finales
        print("\n" + "="*70)
        print("üìä M√âTRICAS DE LA INTERACCI√ìN")
        print("="*70)
        print(f"Categor√≠a: {final_state['category'].upper()}")
        print(f"Urgencia: {final_state['urgency'].upper()}")
        print(f"Confidence Score: {final_state['confidence_score']:.2f}")
        print(f"Escalado a Humano: {'‚úÖ S√ç' if final_state['should_escalate'] else '‚ùå NO'}")
        print(f"KB Results: {len(final_state['kb_results'])} recursos encontrados")

        if i < len(queries):
            input("\n[Presiona Enter para siguiente consulta...]")

    print("\n" + "="*70)
    print("üéâ ¬°DEMO COMPLETADA!")
    print("="*70)
    print("\nüí° Este sistema integra todos los conceptos del tutorial:")
    print("   ‚úÖ Routing basado en clasificaci√≥n (M√≥dulo 2.1)")
    print("   ‚úÖ Agentes especializados con expertise (M√≥dulo 3.2)")
    print("   ‚úÖ Knowledge base como memoria (M√≥dulo 3.3)")
    print("   ‚úÖ S√≠ntesis de m√∫ltiples fuentes (M√≥dulo 2.3)")
    print("   ‚úÖ Decision making con confidence (M√≥dulo 3.1)")
    print("\nüöÄ Listo para producci√≥n con:")
    print("   ‚Ä¢ Vector DB para KB search sem√°ntica")
    print("   ‚Ä¢ Integraci√≥n con CRM/ticketing system")
    print("   ‚Ä¢ Logging y monitoring")
    print("   ‚Ä¢ A/B testing de respuestas")


if __name__ == "__main__":
    main()

"""
Ejercicio 1.2: Agente BÃ¡sico AutÃ³nomo con Herramientas - SOLUCIÃ“N COMPLETA

Este mÃ³dulo implementa un agente que puede:
- Razonar sobre quÃ© herramientas necesita
- Ejecutar herramientas dinÃ¡micamente
- Decidir cuÃ¡ndo ha completado su tarea

Conceptos implementados:
- ReAct pattern (Reasoning + Acting)
- Tool calling y tool binding
- Conditional edges
- Ciclos en grafos
- ToolNode para ejecuciÃ³n de herramientas
"""

from typing import Annotated, Sequence, Literal
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

# Cargar variables de entorno
load_dotenv()

# =============================================================================
# DEFINICIÃ“N DEL ESTADO DEL AGENTE
# =============================================================================

class AgentState(dict):
    """
    Estado del agente con historial de mensajes.

    A diferencia del Ejercicio 1.1 donde usÃ¡bamos campos especÃ­ficos (article,
    summary, etc.), los agentes usan una secuencia de mensajes porque:

    1. Permite al LLM ver todo el contexto de la conversaciÃ³n
    2. Incluye diferentes tipos de mensajes:
       - HumanMessage: Mensajes del usuario
       - AIMessage: Respuestas del agente
       - SystemMessage: Instrucciones del sistema
       - ToolMessage: Resultados de herramientas

    El decorator Annotated con add_messages es un "reducer" especial:
    - AutomÃ¡ticamente agrega nuevos mensajes al final
    - No duplica mensajes con el mismo ID
    - Mantiene el orden cronolÃ³gico
    - Permite al estado evolucionar a travÃ©s del grafo

    Ejemplo de flujo de mensajes:
    [HumanMessage("Â¿cuÃ¡nto es 2+2?")]
    â†’ [HumanMessage("Â¿cuÃ¡nto es 2+2?"), AIMessage(tool_calls=[calculator])]
    â†’ [..., ToolMessage("4")]
    â†’ [..., AIMessage("El resultado es 4")]
    """
    messages: Annotated[Sequence[BaseMessage], add_messages]


# =============================================================================
# DEFINICIÃ“N DE HERRAMIENTAS
# =============================================================================

@tool
def calculator(expression: str) -> str:
    """
    Calcula expresiones matemÃ¡ticas simples.

    El decorator @tool convierte una funciÃ³n Python en una herramienta que:
    1. El LLM puede descubrir y entender
    2. Tiene un esquema JSON automÃ¡tico basado en los type hints
    3. Incluye la docstring como descripciÃ³n para el LLM

    Esta herramienta puede evaluar:
    - Operaciones bÃ¡sicas: +, -, *, /
    - Porcentajes: "15% of 250"
    - Potencias: 2**3
    - ParÃ©ntesis para precedencia

    Args:
        expression: ExpresiÃ³n matemÃ¡tica como string (ej: "2 + 2", "15% of 250")

    Returns:
        Resultado del cÃ¡lculo como string

    Ejemplos:
        calculator("2 + 2") â†’ "4"
        calculator("15% of 250") â†’ "37.5"
        calculator("(10 + 5) * 2") â†’ "30"
    """
    try:
        # Manejar porcentajes: "X% of Y" â†’ (X/100) * Y
        if "%" in expression and "of" in expression:
            parts = expression.replace("%", "").split("of")
            if len(parts) == 2:
                percent = float(parts[0].strip())
                number = float(parts[1].strip())
                result = (percent / 100) * number
                return str(result)

        # Evaluar expresiÃ³n matemÃ¡tica
        # NOTA: En producciÃ³n, usar una librerÃ­a mÃ¡s segura que eval()
        # como simpleeval o crear un parser propio
        result = eval(expression)
        return str(result)
    except Exception as e:
        return f"Error al calcular: {str(e)}"


@tool
def search_knowledge(query: str) -> str:
    """
    Busca informaciÃ³n en una base de conocimiento simulada.

    Esta es una simulaciÃ³n simple de una base de datos.
    En un sistema real, esto podrÃ­a:
    - Consultar una base de datos SQL
    - Buscar en documentos con RAG (Retrieval-Augmented Generation)
    - Llamar a una API externa
    - Consultar un vector store

    Args:
        query: Consulta de bÃºsqueda (ej: "precio producto X")

    Returns:
        InformaciÃ³n encontrada o mensaje de no encontrado

    Ejemplos:
        search_knowledge("precio producto X") â†’ "El precio del producto X es $120"
        search_knowledge("horario tienda") â†’ "La tienda abre de 9:00 a 18:00"
    """
    # Base de conocimiento simulada (en producciÃ³n serÃ­a una DB real)
    knowledge_base = {
        "producto x": "El precio del producto X es $120",
        "producto y": "El precio del producto Y es $85",
        "producto z": "El precio del producto Z es $200",
        "horario": "La tienda abre de 9:00 a 18:00, de lunes a sÃ¡bado",
        "envÃ­o": "El envÃ­o es gratuito para compras superiores a $100",
        "garantÃ­a": "Todos los productos tienen garantÃ­a de 1 aÃ±o",
        "devoluciones": "Aceptamos devoluciones dentro de los 30 dÃ­as",
    }

    # Buscar en la base de conocimiento (bÃºsqueda simple por substring)
    query_lower = query.lower()
    for key, value in knowledge_base.items():
        if key in query_lower:
            return value

    return f"No se encontrÃ³ informaciÃ³n sobre: {query}"


# Lista de herramientas disponibles para el agente
tools = [calculator, search_knowledge]

# =============================================================================
# CONFIGURACIÃ“N DEL LLM CON HERRAMIENTAS
# =============================================================================

# Inicializar el modelo
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,  # Temperatura 0 para razonamiento mÃ¡s determinÃ­stico
)

# Vincular herramientas al LLM
# bind_tools() es un mÃ©todo especial que:
# 1. Convierte las herramientas a formato de OpenAI function calling
# 2. Las incluye en cada llamada al LLM
# 3. Permite al LLM decidir cuÃ¡ndo y cÃ³mo llamarlas
llm_with_tools = llm.bind_tools(tools)

# System prompt que guÃ­a el comportamiento del agente
SYSTEM_PROMPT = """Eres un asistente Ãºtil que puede realizar cÃ¡lculos y buscar informaciÃ³n.

Tienes acceso a las siguientes herramientas:
- calculator: Para realizar cÃ¡lculos matemÃ¡ticos
- search_knowledge: Para buscar informaciÃ³n en la base de conocimiento

Cuando recibas una pregunta:
1. Piensa quÃ© informaciÃ³n necesitas
2. Usa las herramientas apropiadas
3. Una vez que tengas toda la informaciÃ³n, proporciona una respuesta clara

Siempre explica tu razonamiento brevemente."""


# =============================================================================
# DEFINICIÃ“N DE NODOS
# =============================================================================

def agent_node(state: AgentState) -> dict:
    """
    Nodo del agente: razona y decide quÃ© hacer.

    Este es el "cerebro" del agente. En cada llamada:
    1. Recibe todo el historial de mensajes (contexto completo)
    2. El LLM analiza el contexto y decide:
       - OpciÃ³n A: Llamar una o mÃ¡s herramientas (tool_calls)
       - OpciÃ³n B: Responder directamente al usuario
    3. Retorna un AIMessage con su decisiÃ³n

    El LLM ve:
    - El system prompt con instrucciones
    - Todos los mensajes previos (contexto)
    - Las herramientas disponibles (via bind_tools)
    - Los resultados de herramientas previas (ToolMessages)

    Args:
        state: Estado con historial de mensajes

    Returns:
        Diccionario con el nuevo mensaje del agente
    """
    print("\nğŸ¤– Agente pensando...")

    # 1. Obtener mensajes del estado
    messages = state["messages"]

    # 2. Agregar system prompt si es el primer mensaje del usuario
    # Solo lo agregamos una vez al inicio
    if len(messages) == 1:
        messages = [SystemMessage(content=SYSTEM_PROMPT)] + list(messages)

    # 3. Invocar el LLM con herramientas vinculadas
    # El LLM decidirÃ¡ si necesita usar herramientas o responder
    response = llm_with_tools.invoke(messages)

    # 4. Logging para debugging (opcional pero Ãºtil)
    if hasattr(response, "tool_calls") and response.tool_calls:
        print(f"   â†’ El agente quiere usar {len(response.tool_calls)} herramienta(s)")
        for tc in response.tool_calls:
            print(f"      â€¢ {tc['name']}({tc['args']})")
    else:
        print("   â†’ El agente tiene una respuesta final")

    # 5. Retornar el mensaje de respuesta
    # add_messages automÃ¡ticamente lo agregarÃ¡ al historial
    return {"messages": [response]}


def should_continue(state: AgentState) -> Literal["continue", "end"]:
    """
    FunciÃ³n de routing: decide si el agente debe continuar o terminar.

    Esta es una funciÃ³n crucial en el pattern ReAct. Determina:
    - Si el agente quiere usar herramientas â†’ continuar el ciclo
    - Si el agente ya tiene la respuesta â†’ terminar

    El flujo es:
    - agent genera AIMessage con tool_calls â†’ "continue" â†’ ejecutar tools
    - tools generan ToolMessages â†’ automÃ¡tico volver a agent
    - agent genera AIMessage sin tool_calls â†’ "end" â†’ terminar

    Args:
        state: Estado actual

    Returns:
        "continue" si hay herramientas por ejecutar, "end" si debe terminar
    """
    # Obtener el Ãºltimo mensaje (que siempre es del agente)
    last_message = state["messages"][-1]

    # Verificar si el mensaje tiene tool_calls
    # Los modelos que soportan function calling agregan este atributo
    # cuando deciden usar herramientas
    if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
        print("   ğŸ”„ Hay herramientas por ejecutar, continuando...")
        return "continue"
    else:
        print("   âœ… No hay mÃ¡s herramientas, terminando...")
        return "end"


# =============================================================================
# CONSTRUCCIÃ“N DEL GRAFO
# =============================================================================

def build_graph():
    """
    Construye el grafo del agente con ciclo de razonamiento.

    Este grafo implementa el pattern ReAct:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         START                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  agent_node                           â”‚
    â”‚  - Recibe contexto                    â”‚
    â”‚  - LLM decide quÃ© hacer               â”‚
    â”‚  - Retorna AIMessage                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            [should_continue?]
                â†™         â†˜
          "continue"      "end"
             â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    END
    â”‚  tool_node     â”‚
    â”‚  - Ejecuta     â”‚
    â”‚    herramientasâ”‚
    â”‚  - Retorna     â”‚
    â”‚    ToolMessagesâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
          (volver a agent) â†â”€â”
                             â”‚
                           CICLO

    CaracterÃ­sticas importantes:
    1. Es un CICLO: agent â†’ tools â†’ agent â†’ ...
    2. No sabemos cuÃ¡ntas iteraciones tomarÃ¡
    3. El agente decide cuÃ¡ndo terminar
    4. Cada herramienta ejecutada agrega contexto

    Returns:
        Grafo compilado listo para ejecutar
    """
    # 1. Crear el grafo con el tipo de estado
    workflow = StateGraph(AgentState)

    # 2. Crear el nodo de herramientas usando ToolNode
    # ToolNode es una clase proporcionada por LangGraph que:
    # - Extrae tool_calls del Ãºltimo AIMessage
    # - Busca las herramientas correspondientes
    # - Las ejecuta con los argumentos especificados
    # - Retorna ToolMessages con los resultados
    # Esto nos ahorra implementar la lÃ³gica manualmente
    tool_node = ToolNode(tools=tools)

    # 3. Agregar nodos al grafo
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)

    # 4. Establecer el punto de entrada
    # El grafo siempre comienza con el agente
    workflow.set_entry_point("agent")

    # 5. Agregar conditional edge desde el agente
    # DespuÃ©s de que el agente razona, should_continue decide:
    # - "continue" â†’ ejecutar herramientas
    # - "end" â†’ terminar (el agente ya respondiÃ³)
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "continue": "tools",
            "end": END
        }
    )

    # 6. Agregar edge desde tools de vuelta a agent
    # Â¡ESTO CREA EL CICLO!
    # DespuÃ©s de ejecutar herramientas, siempre volvemos al agente
    # para que analice los resultados y decida quÃ© hacer
    workflow.add_edge("tools", "agent")

    # 7. Compilar el grafo
    return workflow.compile()


# =============================================================================
# EJECUCIÃ“N DEL AGENTE
# =============================================================================

def main():
    """
    FunciÃ³n principal que ejecuta el agente con diferentes consultas.

    Demuestra varios casos de uso:
    1. Consulta simple (una herramienta)
    2. Consulta de bÃºsqueda (otra herramienta)
    3. Consulta compleja (mÃºltiples herramientas)
    4. Otra consulta de bÃºsqueda

    Esto muestra la flexibilidad del agente para adaptarse
    a diferentes tipos de tareas sin reprogramaciÃ³n.
    """
    print("\n" + "="*70)
    print("ğŸš€ AGENTE AUTÃ“NOMO CON HERRAMIENTAS")
    print("="*70)

    # Construir el grafo
    app = build_graph()

    # Ejemplos de consultas que requieren diferentes herramientas
    queries = [
        "Â¿CuÃ¡nto es 15% de 250?",
        "Â¿CuÃ¡l es el precio del producto X?",
        "Calcula el 20% de 450 y sÃºmale el precio del producto Y",
        "Â¿CuÃ¡l es el horario de la tienda?",
    ]

    for i, query in enumerate(queries, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“ CONSULTA {i}: {query}")
        print('='*70)

        # Crear estado inicial con la consulta del usuario
        initial_state = {
            "messages": [HumanMessage(content=query)]
        }

        # Ejecutar el agente
        # El mÃ©todo invoke() ejecutarÃ¡ el grafo completo:
        # - Puede hacer mÃºltiples iteraciones del ciclo agent â†’ tools
        # - ContinÃºa hasta que el agente decida terminar
        # - Retorna el estado final con todos los mensajes
        final_state = app.invoke(initial_state)

        # Mostrar la respuesta final (Ãºltimo mensaje)
        final_message = final_state["messages"][-1]
        print(f"\nâœ… RESPUESTA FINAL:")
        print(f"{final_message.content}")
        print()

        # InformaciÃ³n de debugging Ãºtil
        num_steps = len(final_state["messages"])
        print(f"ğŸ“Š EstadÃ­sticas:")
        print(f"   â€¢ Total de mensajes: {num_steps}")
        print(f"   â€¢ Iteraciones aproximadas: {(num_steps - 1) // 2}")

        # PequeÃ±a separaciÃ³n entre consultas
        if i < len(queries):
            input("\nPresiona Enter para continuar con la siguiente consulta...")

    print("\n" + "="*70)
    print("ğŸ‰ Â¡Ejercicio completado! Has creado tu primer agente autÃ³nomo.")
    print("="*70)
    print("\nğŸ’¡ Conceptos clave aprendidos:")
    print("   â€¢ ReAct pattern (Reasoning + Acting)")
    print("   â€¢ Tool calling y binding")
    print("   â€¢ Conditional edges para decisiones dinÃ¡micas")
    print("   â€¢ Ciclos en grafos de LangGraph")
    print("   â€¢ Diferencia entre workflows y agentes")


if __name__ == "__main__":
    main()
